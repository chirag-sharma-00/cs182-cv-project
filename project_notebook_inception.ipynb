{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "project_notebook_inception.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "41ccff8b4aed4f5eb80cc5c632fbe6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3f05986966534fc49aa45c5170525fe6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c6f1b6cf18434bb6b7c11ce5b61696a8",
              "IPY_MODEL_ceb882ee34c544e0a12e068f52da3e76"
            ]
          }
        },
        "3f05986966534fc49aa45c5170525fe6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6f1b6cf18434bb6b7c11ce5b61696a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2e95e5d2c34f4a7d8275ccf6fd8f7a57",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 108857766,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 108857766,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1ddf7f4125a4a1bb767e203bf85aae0"
          }
        },
        "ceb882ee34c544e0a12e068f52da3e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bbe13902cb3f4b9d850b804355921703",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 104M/104M [00:03&lt;00:00, 31.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6dfa25941a640539cfe58a78485925f"
          }
        },
        "2e95e5d2c34f4a7d8275ccf6fd8f7a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1ddf7f4125a4a1bb767e203bf85aae0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbe13902cb3f4b9d850b804355921703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6dfa25941a640539cfe58a78485925f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chirag-sharma-00/cs182-cv-project/blob/main/project_notebook_inception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VOqVl7R0Kuk"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import glob\n",
        "import pathlib\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxdDCtGN0Xdd"
      },
      "source": [
        "!rm -rf sample_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ehVR2p-0Zrp",
        "outputId": "dcdfa7ce-e76b-435a-822c-38ad59f0a143"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDowfYzc0mns"
      },
      "source": [
        "!tar -xf drive/MyDrive/tiny-imagenet-200.tar.xz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17AQlMkS0Kum"
      },
      "source": [
        "## Preliminary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBLZ5xnC0Kun"
      },
      "source": [
        "images = pd.read_csv(\"tiny-imagenet-200/words.txt\", names = ['Id', 'labels'], sep = '\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MM9nrXmm0Kuo"
      },
      "source": [
        "#takes a while to run\n",
        "images_expanded = pd.DataFrame(columns = ['Id', 'label'])\n",
        "for img in images['Id']: \n",
        "    classes = images[images['Id'] == img]['labels']\n",
        "    \n",
        "    for img_class in classes: \n",
        "        images_expanded = images_expanded.append({'Id': img, 'label' : img_class}, ignore_index = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1zrqrZD0Kup"
      },
      "source": [
        "images_expanded.to_csv('words_expanded.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSt6noAM0Kup"
      },
      "source": [
        "data_dir = pathlib.Path('tiny-imagenet-200/')\n",
        "image_count = len(list(data_dir.glob('**/*.JPEG')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLuV3ve10Kup",
        "outputId": "8fa0eaf2-0e61-4bfc-f879-8f04a6da32f4"
      },
      "source": [
        "CLASS_NAMES = np.array([item.name for item in (data_dir / 'train').glob('*')])\n",
        "CLASS_NAMES.sort()\n",
        "sum([cls in images['Id'].unique() for cls in CLASS_NAMES])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OXXBOpW0Kuq"
      },
      "source": [
        "## Testing Inception v3 out of the box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_A59nf40Kur"
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dCrItwU--0L"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsVdHscI0Kur"
      },
      "source": [
        "data_dir = data_dir\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"inception_v3\"\n",
        "# Number of classes in the dataset\n",
        "num_classes = 200\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 8\n",
        "# Number of epochs to train for\n",
        "num_epochs = 3\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "# when True we only update the reshaped layer params\n",
        "feature_extract = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "41ccff8b4aed4f5eb80cc5c632fbe6e7",
            "3f05986966534fc49aa45c5170525fe6",
            "c6f1b6cf18434bb6b7c11ce5b61696a8",
            "ceb882ee34c544e0a12e068f52da3e76",
            "2e95e5d2c34f4a7d8275ccf6fd8f7a57",
            "e1ddf7f4125a4a1bb767e203bf85aae0",
            "bbe13902cb3f4b9d850b804355921703",
            "b6dfa25941a640539cfe58a78485925f"
          ]
        },
        "id": "L_kt9mOh0Kus",
        "outputId": "3fa06c57-4b83-40f1-f135-6aeb1408d1af"
      },
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.9.0', 'inception_v3', pretrained=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/archive/v0.9.0.zip\" to /root/.cache/torch/hub/v0.9.0.zip\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41ccff8b4aed4f5eb80cc5c632fbe6e7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-7BA0Ej0Kut"
      },
      "source": [
        "#change last layers in model to match tiny imagenet\n",
        "model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
        "model.fc = nn.Linear(2048, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKmbeEje_mTr"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Y0EL_b0Kut"
      },
      "source": [
        "#inception expects input size 3*299*299\n",
        "input_size = 299"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Myt8w-EP0Kuu"
      },
      "source": [
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WP8VVyF0Kuu"
      },
      "source": [
        "#test on one image\n",
        "input_image_one = Image.open(\"tiny-imagenet-200/train/n01443537/images/n01443537_0.JPEG\")\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(299),\n",
        "    transforms.CenterCrop(299),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "#converts image to 3*299*299\n",
        "input_tensor_one = preprocess(input_image_one)\n",
        "\n",
        "#convert to batches\n",
        "input_batch = torch.cat((input_tensor_one.unsqueeze(0), input_tensor_one.unsqueeze(0)))\n",
        "input_batch = input_batch.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJP9O1jm0Kuu",
        "outputId": "0884d038-352f-4816-f876-2e44f8665c31"
      },
      "source": [
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "print(probabilities)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6637, 0.4933, 0.4839, 0.4828, 0.5951, 0.4830, 0.4790, 0.3699, 0.4196,\n",
            "         0.4716, 0.5252, 0.4348, 0.5166, 0.3835, 0.6465, 0.4121, 0.4759, 0.5003,\n",
            "         0.5494, 0.5972, 0.5674, 0.4990, 0.6205, 0.5291, 0.4867, 0.5412, 0.5171,\n",
            "         0.5869, 0.5255, 0.5191, 0.4546, 0.3775, 0.5631, 0.5286, 0.5682, 0.4727,\n",
            "         0.4739, 0.4929, 0.5127, 0.5284, 0.5206, 0.4834, 0.4205, 0.4837, 0.5085,\n",
            "         0.5380, 0.4609, 0.5679, 0.4848, 0.5580, 0.6147, 0.4397, 0.4967, 0.5189,\n",
            "         0.5783, 0.5649, 0.5924, 0.4191, 0.4089, 0.3955, 0.5462, 0.3598, 0.5634,\n",
            "         0.5617, 0.4330, 0.6110, 0.4566, 0.5135, 0.5771, 0.5243, 0.4364, 0.4826,\n",
            "         0.5339, 0.4466, 0.5800, 0.4521, 0.5315, 0.4604, 0.4775, 0.5296, 0.4712,\n",
            "         0.4664, 0.5415, 0.4248, 0.3972, 0.5106, 0.5120, 0.5031, 0.4978, 0.5566,\n",
            "         0.3899, 0.4492, 0.5022, 0.4175, 0.5557, 0.4461, 0.5699, 0.4395, 0.5105,\n",
            "         0.4616, 0.4858, 0.4421, 0.5587, 0.4735, 0.5379, 0.5235, 0.6288, 0.4668,\n",
            "         0.5342, 0.5073, 0.6045, 0.4508, 0.4706, 0.4642, 0.5310, 0.4492, 0.5665,\n",
            "         0.5026, 0.6753, 0.3995, 0.4899, 0.3680, 0.5145, 0.4707, 0.5127, 0.5029,\n",
            "         0.5399, 0.4072, 0.4694, 0.4588, 0.5711, 0.5921, 0.5086, 0.5137, 0.4708,\n",
            "         0.5679, 0.4885, 0.4899, 0.5511, 0.4819, 0.4242, 0.5499, 0.4338, 0.4859,\n",
            "         0.4556, 0.5127, 0.5978, 0.4805, 0.3937, 0.4621, 0.5579, 0.3618, 0.5371,\n",
            "         0.6093, 0.3559, 0.4437, 0.5892, 0.5432, 0.3850, 0.5240, 0.4036, 0.5304,\n",
            "         0.4617, 0.6326, 0.6578, 0.6250, 0.5086, 0.5654, 0.6000, 0.4933, 0.4303,\n",
            "         0.5017, 0.5056, 0.4636, 0.5289, 0.4912, 0.4589, 0.5161, 0.4436, 0.6194,\n",
            "         0.5606, 0.4371, 0.4637, 0.4373, 0.4165, 0.5140, 0.5388, 0.5317, 0.6438,\n",
            "         0.5384, 0.5248, 0.5052, 0.5403, 0.4820, 0.4933, 0.6479, 0.4640, 0.5144,\n",
            "         0.5313, 0.4862],\n",
            "        [0.3363, 0.5067, 0.5161, 0.5172, 0.4049, 0.5170, 0.5210, 0.6301, 0.5804,\n",
            "         0.5284, 0.4748, 0.5652, 0.4834, 0.6165, 0.3535, 0.5879, 0.5241, 0.4997,\n",
            "         0.4506, 0.4028, 0.4326, 0.5010, 0.3795, 0.4709, 0.5133, 0.4588, 0.4829,\n",
            "         0.4131, 0.4745, 0.4809, 0.5454, 0.6225, 0.4369, 0.4714, 0.4318, 0.5273,\n",
            "         0.5261, 0.5071, 0.4873, 0.4716, 0.4794, 0.5166, 0.5795, 0.5163, 0.4915,\n",
            "         0.4620, 0.5391, 0.4321, 0.5152, 0.4420, 0.3853, 0.5603, 0.5033, 0.4811,\n",
            "         0.4217, 0.4351, 0.4076, 0.5809, 0.5911, 0.6045, 0.4538, 0.6402, 0.4366,\n",
            "         0.4383, 0.5670, 0.3890, 0.5434, 0.4865, 0.4229, 0.4757, 0.5636, 0.5174,\n",
            "         0.4661, 0.5534, 0.4200, 0.5479, 0.4685, 0.5396, 0.5225, 0.4704, 0.5288,\n",
            "         0.5336, 0.4585, 0.5752, 0.6028, 0.4894, 0.4880, 0.4969, 0.5022, 0.4434,\n",
            "         0.6101, 0.5508, 0.4978, 0.5825, 0.4443, 0.5539, 0.4301, 0.5605, 0.4895,\n",
            "         0.5384, 0.5142, 0.5579, 0.4413, 0.5265, 0.4621, 0.4765, 0.3712, 0.5332,\n",
            "         0.4658, 0.4927, 0.3955, 0.5492, 0.5294, 0.5358, 0.4690, 0.5508, 0.4335,\n",
            "         0.4974, 0.3247, 0.6005, 0.5101, 0.6320, 0.4855, 0.5293, 0.4873, 0.4971,\n",
            "         0.4601, 0.5928, 0.5306, 0.5412, 0.4289, 0.4079, 0.4914, 0.4863, 0.5292,\n",
            "         0.4321, 0.5115, 0.5101, 0.4489, 0.5181, 0.5758, 0.4501, 0.5662, 0.5141,\n",
            "         0.5444, 0.4873, 0.4022, 0.5195, 0.6063, 0.5379, 0.4421, 0.6382, 0.4629,\n",
            "         0.3907, 0.6441, 0.5563, 0.4108, 0.4568, 0.6150, 0.4760, 0.5964, 0.4696,\n",
            "         0.5383, 0.3674, 0.3422, 0.3750, 0.4914, 0.4346, 0.4000, 0.5067, 0.5697,\n",
            "         0.4983, 0.4944, 0.5364, 0.4711, 0.5088, 0.5411, 0.4839, 0.5564, 0.3806,\n",
            "         0.4394, 0.5629, 0.5363, 0.5627, 0.5835, 0.4860, 0.4612, 0.4683, 0.3562,\n",
            "         0.4616, 0.4752, 0.4948, 0.4597, 0.5180, 0.5067, 0.3521, 0.5360, 0.4856,\n",
            "         0.4687, 0.5138]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAZ7Ufuc0Kuv",
        "outputId": "c5290c7e-76fe-4987-f902-0e2431080b27"
      },
      "source": [
        "torch.topk(probabilities[0], 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(values=tensor([0.6753, 0.6637, 0.6578, 0.6479, 0.6465], device='cuda:0'), indices=tensor([118,   0, 164, 195,  14], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec1AWLkc0Kuv"
      },
      "source": [
        "labels = pd.read_csv(\"tiny-imagenet-200/wnids.txt\", names = ['class_labels'], sep = '\\t')\n",
        "labels['predictions'] = probabilities[0].cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "fkQxzNPm0Kuv",
        "outputId": "4f6da21e-88c2-4154-dd4f-062dd99dea74"
      },
      "source": [
        "labels.sort_values(by = ['predictions'], ascending = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class_labels</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>n04560804</td>\n",
              "      <td>0.675302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n02124075</td>\n",
              "      <td>0.663655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>n01917289</td>\n",
              "      <td>0.657767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>n12267677</td>\n",
              "      <td>0.647887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>n02132136</td>\n",
              "      <td>0.646546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>n09246464</td>\n",
              "      <td>0.369855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>n07615774</td>\n",
              "      <td>0.368008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>n01983481</td>\n",
              "      <td>0.361850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>n03637318</td>\n",
              "      <td>0.359774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>n03255030</td>\n",
              "      <td>0.355882</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    class_labels  predictions\n",
              "118    n04560804     0.675302\n",
              "0      n02124075     0.663655\n",
              "164    n01917289     0.657767\n",
              "195    n12267677     0.647887\n",
              "14     n02132136     0.646546\n",
              "..           ...          ...\n",
              "7      n09246464     0.369855\n",
              "121    n07615774     0.368008\n",
              "151    n01983481     0.361850\n",
              "61     n03637318     0.359774\n",
              "154    n03255030     0.355882\n",
              "\n",
              "[200 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vISDVvN0Kuw"
      },
      "source": [
        "### Updating feature extraction to match our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKFgtKgi0Kuw"
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=1, is_inception=True):\n",
        "    if torch.cuda.is_available():\n",
        "      model = model.cuda()\n",
        "      \n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training (no val for now)\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "\n",
        "                # move to gpu\n",
        "                inputs = inputs.to(device)\n",
        "                labels.data = labels.data.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUXDW_3u0Kux"
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting=True):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB1g-Zwt0Kuz"
      },
      "source": [
        "def predict(dataloaders, model): \n",
        "    \"\"\"\n",
        "    Run a forward pass (without caching data) for given model and return accuracy\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "      model = model.cuda()\n",
        "\n",
        "    accuracies = []\n",
        "    model.eval()\n",
        "    \n",
        "    for phase in tqdm.tqdm(['train', 'val']):\n",
        "        counter = 0\n",
        "        running_corrects = 0\n",
        "        running_total = 0\n",
        "\n",
        "        for inputs, labels in tqdm.tqdm(dataloaders[phase]): \n",
        "            inputs = inputs.to(device)\n",
        "            labels.data = labels.data.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            counter += 1\n",
        "            \n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            running_total += len(preds)\n",
        "\n",
        "            if counter == 100:\n",
        "              break\n",
        "        phase_acc = running_corrects / running_total\n",
        "        print(phase_acc)\n",
        "        accuracies.append(phase_acc)\n",
        "\n",
        "    return accuracies\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH4KpiHl0Ku0"
      },
      "source": [
        "##inception model to finetune\n",
        "model_ft = models.inception_v3(pretrained=True)\n",
        "set_parameter_requires_grad(model_ft, feature_extract)\n",
        "# Handle the auxilary net, requires_grads automatically set to true \n",
        "num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "# Handle the primary net, requires_grad automatically set to true \n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "input_size = 299"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soHwV5kW0Ku0"
      },
      "source": [
        "##out of the box model\n",
        "scratch_model = models.inception_v3(pretrained=True)\n",
        "scratch_model.fc = nn.Linear(scratch_model.fc.in_features, 200)\n",
        "scratch_model.AuxLogits.fc = nn.Linear(scratch_model.AuxLogits.fc.in_features, num_classes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZw5XXzf0Ku1"
      },
      "source": [
        "#### Check accuracy of out of the box model on all labeled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PbCT5mt0Ku1"
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Custom dataloader based on https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "class ValidationDataset(Dataset):\n",
        "  def __init__(self, annotations_file, img_dir, transform=None):\n",
        "    self.img_labels = pd.read_csv(annotations_file, sep='\\t', names=['image', 'label', 'x1', 'y1', 'x2', 'y2'])\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "    image = Image.open(img_path)\n",
        "    image = image.convert('RGB')\n",
        "    label = self.img_labels.iloc[idx, 1]\n",
        "    label = np.where(label==CLASS_NAMES)[0][0]\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    return image, label\n",
        "\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transforms['train']),\n",
        "    'val': ValidationDataset(os.path.join(data_dir, 'val', 'val_annotations.txt'), os.path.join(data_dir, 'val', 'images'), data_transforms['val'])\n",
        "}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=2) for x in ['train', 'val']}"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-4-SSka0Ku2",
        "outputId": "30d0f6c1-f098-421f-89ae-ad344453557a"
      },
      "source": [
        "out_box_acc = predict(dataloaders_dict, scratch_model)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]\n",
            "  0%|          | 0/12500 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/12500 [00:00<37:18,  5.58it/s]\u001b[A\n",
            "  0%|          | 3/12500 [00:00<29:17,  7.11it/s]\u001b[A\n",
            "  0%|          | 6/12500 [00:00<23:18,  8.94it/s]\u001b[A\n",
            "  0%|          | 9/12500 [00:00<18:35, 11.20it/s]\u001b[A\n",
            "  0%|          | 12/12500 [00:00<15:24, 13.51it/s]\u001b[A\n",
            "  0%|          | 15/12500 [00:00<13:01, 15.97it/s]\u001b[A\n",
            "  0%|          | 18/12500 [00:00<11:26, 18.18it/s]\u001b[A\n",
            "  0%|          | 21/12500 [00:00<10:15, 20.27it/s]\u001b[A\n",
            "  0%|          | 24/12500 [00:01<09:30, 21.88it/s]\u001b[A\n",
            "  0%|          | 27/12500 [00:01<09:02, 22.99it/s]\u001b[A\n",
            "  0%|          | 30/12500 [00:01<08:44, 23.78it/s]\u001b[A\n",
            "  0%|          | 33/12500 [00:01<08:31, 24.38it/s]\u001b[A\n",
            "  0%|          | 36/12500 [00:01<08:19, 24.95it/s]\u001b[A\n",
            "  0%|          | 39/12500 [00:01<08:10, 25.40it/s]\u001b[A\n",
            "  0%|          | 42/12500 [00:01<07:58, 26.06it/s]\u001b[A\n",
            "  0%|          | 45/12500 [00:01<07:52, 26.39it/s]\u001b[A\n",
            "  0%|          | 48/12500 [00:01<07:56, 26.14it/s]\u001b[A\n",
            "  0%|          | 51/12500 [00:02<07:49, 26.51it/s]\u001b[A\n",
            "  0%|          | 54/12500 [00:02<07:48, 26.58it/s]\u001b[A\n",
            "  0%|          | 57/12500 [00:02<07:49, 26.48it/s]\u001b[A\n",
            "  0%|          | 60/12500 [00:02<07:51, 26.39it/s]\u001b[A\n",
            "  1%|          | 63/12500 [00:02<07:52, 26.30it/s]\u001b[A\n",
            "  1%|          | 66/12500 [00:02<07:46, 26.64it/s]\u001b[A\n",
            "  1%|          | 69/12500 [00:02<07:43, 26.81it/s]\u001b[A\n",
            "  1%|          | 72/12500 [00:02<07:45, 26.71it/s]\u001b[A\n",
            "  1%|          | 75/12500 [00:03<07:50, 26.41it/s]\u001b[A\n",
            "  1%|          | 78/12500 [00:03<07:48, 26.53it/s]\u001b[A\n",
            "  1%|          | 81/12500 [00:03<07:47, 26.55it/s]\u001b[A\n",
            "  1%|          | 84/12500 [00:03<07:48, 26.52it/s]\u001b[A\n",
            "  1%|          | 87/12500 [00:03<07:53, 26.20it/s]\u001b[A\n",
            "  1%|          | 90/12500 [00:03<07:47, 26.57it/s]\u001b[A\n",
            "  1%|          | 93/12500 [00:03<07:43, 26.79it/s]\u001b[A\n",
            "  1%|          | 96/12500 [00:03<07:46, 26.57it/s]\u001b[A\n",
            "  1%|          | 99/12500 [00:03<08:20, 24.78it/s]\n",
            " 50%|█████     | 1/2 [00:04<00:04,  4.01s/it]\n",
            "  0%|          | 0/1250 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0037, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 1/1250 [00:00<03:39,  5.70it/s]\u001b[A\n",
            "  0%|          | 4/1250 [00:00<02:48,  7.40it/s]\u001b[A\n",
            "  1%|          | 7/1250 [00:00<02:10,  9.49it/s]\u001b[A\n",
            "  1%|          | 10/1250 [00:00<01:45, 11.70it/s]\u001b[A\n",
            "  1%|          | 13/1250 [00:00<01:28, 14.02it/s]\u001b[A\n",
            "  1%|▏         | 16/1250 [00:00<01:15, 16.42it/s]\u001b[A\n",
            "  2%|▏         | 19/1250 [00:00<01:06, 18.38it/s]\u001b[A\n",
            "  2%|▏         | 22/1250 [00:00<01:01, 19.87it/s]\u001b[A\n",
            "  2%|▏         | 25/1250 [00:01<00:56, 21.64it/s]\u001b[A\n",
            "  2%|▏         | 28/1250 [00:01<00:53, 22.83it/s]\u001b[A\n",
            "  2%|▏         | 31/1250 [00:01<00:51, 23.72it/s]\u001b[A\n",
            "  3%|▎         | 34/1250 [00:01<00:49, 24.78it/s]\u001b[A\n",
            "  3%|▎         | 37/1250 [00:01<00:47, 25.28it/s]\u001b[A\n",
            "  3%|▎         | 40/1250 [00:01<00:46, 25.90it/s]\u001b[A\n",
            "  3%|▎         | 43/1250 [00:01<00:46, 26.07it/s]\u001b[A\n",
            "  4%|▎         | 46/1250 [00:01<00:45, 26.28it/s]\u001b[A\n",
            "  4%|▍         | 49/1250 [00:02<00:46, 26.06it/s]\u001b[A\n",
            "  4%|▍         | 52/1250 [00:02<00:46, 26.01it/s]\u001b[A\n",
            "  4%|▍         | 55/1250 [00:02<00:45, 26.14it/s]\u001b[A\n",
            "  5%|▍         | 58/1250 [00:02<00:45, 26.08it/s]\u001b[A\n",
            "  5%|▍         | 61/1250 [00:02<00:46, 25.64it/s]\u001b[A\n",
            "  5%|▌         | 64/1250 [00:02<00:45, 26.05it/s]\u001b[A\n",
            "  5%|▌         | 67/1250 [00:02<00:45, 26.24it/s]\u001b[A\n",
            "  6%|▌         | 70/1250 [00:02<00:45, 26.22it/s]\u001b[A\n",
            "  6%|▌         | 73/1250 [00:02<00:44, 26.39it/s]\u001b[A\n",
            "  6%|▌         | 76/1250 [00:03<00:44, 26.13it/s]\u001b[A\n",
            "  6%|▋         | 79/1250 [00:03<00:44, 26.10it/s]\u001b[A\n",
            "  7%|▋         | 82/1250 [00:03<00:44, 26.27it/s]\u001b[A\n",
            "  7%|▋         | 85/1250 [00:03<00:44, 25.97it/s]\u001b[A\n",
            "  7%|▋         | 88/1250 [00:03<00:44, 26.04it/s]\u001b[A\n",
            "  7%|▋         | 91/1250 [00:03<00:44, 26.13it/s]\u001b[A\n",
            "  8%|▊         | 94/1250 [00:03<00:44, 26.21it/s]\u001b[A\n",
            "  8%|▊         | 97/1250 [00:03<00:47, 24.30it/s]\n",
            "100%|██████████| 2/2 [00:08<00:00,  4.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0050, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVoOXbkw0Ku3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860d0f31-c391-4970-ceb3-77905b846a0d"
      },
      "source": [
        "out_box_acc"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(0.0037, device='cuda:0'), tensor(0.0050, device='cuda:0')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n6VwVUp0Ku3"
      },
      "source": [
        "#### Retrain inception on transformed data and check accuracy on train/val data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWkuo8NI0Ku3"
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transforms['train']),\n",
        "    'val': ValidationDataset(os.path.join(data_dir, 'val', 'val_annotations.txt'), os.path.join(data_dir, 'val', 'images'), data_transforms['val'])\n",
        "}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=2) for x in ['train', 'val']}"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHrbET6I0Ku3",
        "outputId": "5ae54e51-64de-4777-b135-2003ecd678a4"
      },
      "source": [
        "params_to_update = []\n",
        "for name,param in model_ft.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t AuxLogits.fc.weight\n",
            "\t AuxLogits.fc.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6PFJNny0Ku4",
        "scrolled": true,
        "outputId": "e86e5886-2dd7-4093-e2b2-4cfbbfbc31bb"
      },
      "source": [
        "##finetuned model accuracy\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/2\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyFiQQEz2SL6"
      },
      "source": [
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transforms['train']),\n",
        "    'val': ValidationDataset(os.path.join(data_dir, 'val', 'val_annotations.txt'), os.path.join(data_dir, 'val', 'images'), data_transforms['val'])\n",
        "}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=2) for x in ['train', 'val']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBkZ83iw2aAf",
        "outputId": "74974058-eaf4-44b5-f2c5-15e6c21d16b7"
      },
      "source": [
        "params_to_update = []\n",
        "for name,param in model_ft.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t AuxLogits.fc.weight\n",
            "\t AuxLogits.fc.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkY_-2Gw2cMp",
        "outputId": "1d264b20-b499-41fe-aaea-a8645a7eec27"
      },
      "source": [
        "##finetuned model accuracy\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/2\n",
            "----------\n",
            "train Loss: 5.1868 Acc: 0.2296\n",
            "val Loss: 3.1657 Acc: 0.3264\n",
            "\n",
            "Epoch 1/2\n",
            "----------\n",
            "train Loss: 5.0573 Acc: 0.2375\n",
            "val Loss: 3.1471 Acc: 0.3283\n",
            "\n",
            "Epoch 2/2\n",
            "----------\n",
            "train Loss: 5.0072 Acc: 0.2400\n",
            "val Loss: 3.1676 Acc: 0.3188\n",
            "\n",
            "Training complete in 28m 9s\n",
            "Best val Acc: 0.328300\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
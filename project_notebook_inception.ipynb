{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pd.read_csv(\"../data/tiny-imagenet-200/words.txt\", names = ['Id', 'labels'], sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3bf98b5cfeb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimages_expanded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimg_class\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun jadia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2895\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\varun jadia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2948\u001b[0m         \u001b[1;31m# be reindexed to match DataFrame rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2949\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2950\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2951\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#takes a while to run\n",
    "images_expanded = pd.DataFrame(columns = ['Id', 'label'])\n",
    "for img in images['Id']: \n",
    "    classes = images[images['Id'] == img]['labels']\n",
    "    \n",
    "    for img_class in classes: \n",
    "        images_expanded = images_expanded.append({'Id': img, 'label' : img_class}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_expanded.to_csv('words_expande.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('../data/tiny-imagenet-200')\n",
    "image_count = len(list(data_dir.glob('**/*.JPEG')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_NAMES = np.array([item.name for item in (data_dir / 'train').glob('*')])\n",
    "sum([cls in images['Id'].unique() for cls in CLASS_NAMES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Inception v3 out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = data_dir\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"inception_v3\"\n",
    "# Number of classes in the dataset\n",
    "num_classes = 200\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "# Number of epochs to train for\n",
    "num_epochs = 3\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "# when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Varun Jadia/.cache\\torch\\hub\\pytorch_vision_v0.9.0\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.9.0', 'inception_v3', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change last layers in model to match tiny imagenet\n",
    "model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
    "model.fc = nn.Linear(2048, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inception expects input size 3*299*299\n",
    "input_size = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on one image\n",
    "input_image_one = Image.open(\"../data/tiny-imagenet-200/train/n01443537/images/n01443537_0.JPEG\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#converts image to 3*299*299\n",
    "input_tensor_one = preprocess(input_image_one)\n",
    "\n",
    "#convert to batches\n",
    "input_batch = torch.cat((input_tensor_one.unsqueeze(0), input_tensor_one.unsqueeze(0)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5844, 0.5588, 0.3677, 0.5314, 0.5706, 0.6019, 0.4203, 0.4587, 0.4384,\n",
      "         0.4523, 0.5917, 0.3619, 0.5725, 0.5263, 0.5770, 0.4273, 0.5186, 0.4732,\n",
      "         0.4843, 0.4914, 0.4898, 0.5532, 0.3608, 0.4117, 0.5177, 0.4508, 0.4183,\n",
      "         0.4943, 0.3829, 0.5474, 0.4819, 0.4926, 0.3652, 0.5660, 0.5301, 0.4787,\n",
      "         0.5190, 0.5015, 0.4638, 0.4591, 0.5849, 0.5683, 0.5857, 0.4282, 0.4621,\n",
      "         0.4564, 0.4820, 0.4796, 0.6487, 0.4802, 0.4647, 0.4301, 0.5059, 0.5488,\n",
      "         0.4507, 0.4437, 0.5217, 0.5219, 0.5793, 0.4818, 0.4189, 0.5636, 0.5038,\n",
      "         0.4650, 0.5408, 0.5289, 0.4281, 0.5002, 0.4047, 0.4398, 0.4407, 0.4156,\n",
      "         0.5662, 0.3598, 0.5238, 0.4666, 0.5090, 0.5307, 0.6265, 0.4710, 0.5150,\n",
      "         0.4765, 0.4898, 0.5103, 0.4711, 0.5628, 0.5575, 0.4731, 0.4463, 0.4282,\n",
      "         0.4772, 0.5152, 0.4514, 0.3888, 0.4413, 0.5423, 0.3222, 0.4871, 0.4790,\n",
      "         0.5980, 0.5976, 0.5118, 0.5747, 0.6341, 0.5709, 0.3892, 0.4369, 0.4421,\n",
      "         0.5213, 0.5090, 0.6242, 0.4180, 0.3945, 0.4812, 0.3436, 0.3842, 0.4814,\n",
      "         0.5393, 0.5425, 0.5647, 0.4958, 0.4911, 0.5228, 0.4422, 0.4616, 0.4947,\n",
      "         0.5018, 0.5346, 0.5036, 0.5664, 0.5738, 0.5435, 0.4918, 0.5150, 0.5483,\n",
      "         0.5410, 0.5535, 0.4405, 0.5338, 0.5302, 0.4734, 0.5221, 0.3667, 0.4517,\n",
      "         0.5259, 0.4781, 0.4061, 0.4777, 0.4295, 0.4492, 0.4797, 0.4575, 0.4318,\n",
      "         0.5565, 0.5446, 0.4901, 0.5642, 0.5287, 0.5146, 0.3734, 0.5192, 0.3645,\n",
      "         0.5425, 0.4767, 0.3675, 0.5026, 0.5658, 0.4136, 0.4440, 0.5438, 0.5650,\n",
      "         0.4941, 0.4934, 0.4384, 0.4429, 0.5729, 0.4555, 0.5169, 0.4107, 0.5615,\n",
      "         0.5264, 0.4046, 0.6243, 0.5272, 0.5200, 0.5202, 0.4667, 0.5179, 0.6073,\n",
      "         0.5679, 0.4803, 0.4520, 0.5419, 0.4098, 0.4231, 0.4069, 0.5049, 0.5019,\n",
      "         0.4856, 0.5361],\n",
      "        [0.4156, 0.4412, 0.6323, 0.4686, 0.4294, 0.3981, 0.5797, 0.5413, 0.5616,\n",
      "         0.5477, 0.4083, 0.6381, 0.4275, 0.4737, 0.4230, 0.5727, 0.4814, 0.5268,\n",
      "         0.5157, 0.5086, 0.5102, 0.4468, 0.6392, 0.5883, 0.4823, 0.5492, 0.5817,\n",
      "         0.5057, 0.6171, 0.4526, 0.5181, 0.5074, 0.6348, 0.4340, 0.4699, 0.5213,\n",
      "         0.4810, 0.4985, 0.5362, 0.5409, 0.4151, 0.4317, 0.4143, 0.5718, 0.5379,\n",
      "         0.5436, 0.5180, 0.5204, 0.3513, 0.5198, 0.5353, 0.5699, 0.4941, 0.4512,\n",
      "         0.5493, 0.5563, 0.4783, 0.4781, 0.4207, 0.5182, 0.5811, 0.4364, 0.4962,\n",
      "         0.5350, 0.4592, 0.4711, 0.5719, 0.4998, 0.5953, 0.5602, 0.5593, 0.5844,\n",
      "         0.4338, 0.6402, 0.4762, 0.5334, 0.4910, 0.4693, 0.3735, 0.5290, 0.4850,\n",
      "         0.5235, 0.5102, 0.4897, 0.5289, 0.4372, 0.4425, 0.5269, 0.5537, 0.5718,\n",
      "         0.5228, 0.4848, 0.5486, 0.6112, 0.5587, 0.4577, 0.6778, 0.5129, 0.5210,\n",
      "         0.4020, 0.4024, 0.4882, 0.4253, 0.3659, 0.4291, 0.6108, 0.5631, 0.5579,\n",
      "         0.4787, 0.4910, 0.3758, 0.5820, 0.6055, 0.5188, 0.6564, 0.6158, 0.5186,\n",
      "         0.4607, 0.4575, 0.4353, 0.5042, 0.5089, 0.4772, 0.5578, 0.5384, 0.5053,\n",
      "         0.4982, 0.4654, 0.4964, 0.4336, 0.4262, 0.4565, 0.5082, 0.4850, 0.4517,\n",
      "         0.4590, 0.4465, 0.5595, 0.4662, 0.4698, 0.5266, 0.4779, 0.6333, 0.5483,\n",
      "         0.4741, 0.5219, 0.5939, 0.5223, 0.5705, 0.5508, 0.5203, 0.5425, 0.5682,\n",
      "         0.4435, 0.4554, 0.5099, 0.4358, 0.4713, 0.4854, 0.6266, 0.4808, 0.6355,\n",
      "         0.4575, 0.5233, 0.6325, 0.4974, 0.4342, 0.5864, 0.5560, 0.4562, 0.4350,\n",
      "         0.5059, 0.5066, 0.5616, 0.5571, 0.4271, 0.5445, 0.4831, 0.5893, 0.4385,\n",
      "         0.4736, 0.5954, 0.3757, 0.4728, 0.4800, 0.4798, 0.5333, 0.4821, 0.3927,\n",
      "         0.4321, 0.5197, 0.5480, 0.4581, 0.5902, 0.5769, 0.5931, 0.4951, 0.4981,\n",
      "         0.5144, 0.4639]])\n"
     ]
    }
   ],
   "source": [
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.6487, 0.6341, 0.6265, 0.6243, 0.6242]),\n",
       "indices=tensor([ 48, 103,  78, 182, 110]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(probabilities[0], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"../data/tiny-imagenet-200/wnids.txt\", names = ['class_labels'], sep = '\\t')\n",
    "labels['predictions'] = probabilities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_labels</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>n02233338</td>\n",
       "      <td>0.648720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>n03126707</td>\n",
       "      <td>0.634147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>n02099601</td>\n",
       "      <td>0.626471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>n02085620</td>\n",
       "      <td>0.624325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>n03014705</td>\n",
       "      <td>0.624234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>n02106662</td>\n",
       "      <td>0.361941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>n01443537</td>\n",
       "      <td>0.360817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>n02423022</td>\n",
       "      <td>0.359763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>n03670208</td>\n",
       "      <td>0.343643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>n03992509</td>\n",
       "      <td>0.322247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    class_labels  predictions\n",
       "48     n02233338     0.648720\n",
       "103    n03126707     0.634147\n",
       "78     n02099601     0.626471\n",
       "182    n02085620     0.624325\n",
       "110    n03014705     0.624234\n",
       "..           ...          ...\n",
       "11     n02106662     0.361941\n",
       "22     n01443537     0.360817\n",
       "73     n02423022     0.359763\n",
       "114    n03670208     0.343643\n",
       "96     n03992509     0.322247\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.sort_values(by = ['predictions'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating feature extraction to match our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=1, is_inception=True):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training (no val for now)\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting=True):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataloaders, model): \n",
    "    \"\"\"\n",
    "    Run a forward pass (without caching data) for given model and return accuracy\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    \n",
    "    for phase in ['train', 'val']: \n",
    "        for inputs, labels in dataloaders[phase]: \n",
    "            outputs, aux_outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "        phase_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "        print('Acc: {:.4f}'.format(phase, phase_acc))\n",
    "        accuracies.append(phase_acc)\n",
    "\n",
    "    return accuracies\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##inception model to finetune\n",
    "model_ft = models.inception_v3(pretrained=True)\n",
    "set_parameter_requires_grad(model_ft, feature_extract)\n",
    "# Handle the auxilary net, requires_grads automatically set to true \n",
    "num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "# Handle the primary net, requires_grad automatically set to true \n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "input_size = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##out of the box model\n",
    "scratch_model = models.inception_v3(pretrained=True)\n",
    "scratch_model.fc = nn.Linear(scratch_model.fc.in_features, 200)\n",
    "scratch_model.AuxLogits.fc = nn.Linear(scratch_model.AuxLogits.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check accuracy of out of the box model on all labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_accuracies = out_box_acc = predict(dataloaders_dict, scratch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrain inception on transformed data and check accuracy on train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t AuxLogits.fc.weight\n",
      "\t AuxLogits.fc.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "params_to_update = []\n",
    "for name,param in model_ft.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "##finetuned model accuracy\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

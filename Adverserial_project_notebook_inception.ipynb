{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of project_notebook_inception.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8db88b2d610f4bcca6687cd5f3b7b644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6a63d66c1bbe43bbb7f5fd8fe14f10be",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_67f0ab31af62465eb7a3267ba572f9f1",
              "IPY_MODEL_88e01034a6864070901296630b4f2495"
            ]
          }
        },
        "6a63d66c1bbe43bbb7f5fd8fe14f10be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67f0ab31af62465eb7a3267ba572f9f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e550025a59a347f184129038f56da8bd",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 108857766,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 108857766,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_421c06cf93844b85b9648aafc24e8ae5"
          }
        },
        "88e01034a6864070901296630b4f2495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cbbdcec1516f48d78bab6d2dadfdfe2e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 104M/104M [00:13&lt;00:00, 8.09MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b8ecba4b76304b2d8aad2ea6fa054e5a"
          }
        },
        "e550025a59a347f184129038f56da8bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "421c06cf93844b85b9648aafc24e8ae5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cbbdcec1516f48d78bab6d2dadfdfe2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b8ecba4b76304b2d8aad2ea6fa054e5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a2c54a1960154758a06eeffe208f21b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bb7ad17fcbe54229a3fa5318ebf4cb0b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_365c6c1468cb4432af3de3aa0d0345a2",
              "IPY_MODEL_ba6bdeba944e42198840898651c732c5"
            ]
          }
        },
        "bb7ad17fcbe54229a3fa5318ebf4cb0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "365c6c1468cb4432af3de3aa0d0345a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f9dbb203bdb041e6bbd64ba6af899a33",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 108857766,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 108857766,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b3f1f8b6a2d94aac8716a074377fd11d"
          }
        },
        "ba6bdeba944e42198840898651c732c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_793165e771684799be194b72048ac6b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 104M/104M [01:05&lt;00:00, 1.66MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a1cd452241d94440b7b8e54106790e07"
          }
        },
        "f9dbb203bdb041e6bbd64ba6af899a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b3f1f8b6a2d94aac8716a074377fd11d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "793165e771684799be194b72048ac6b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a1cd452241d94440b7b8e54106790e07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "929804dc09e746f08672c4c6c9b9de17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_39ebbdf8ad2942adb49b27d1a14d8b7e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6c345bbcd2aa44609a2fc1184810432e",
              "IPY_MODEL_04058d4e69a5449e96762a5d6aa830d3"
            ]
          }
        },
        "39ebbdf8ad2942adb49b27d1a14d8b7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c345bbcd2aa44609a2fc1184810432e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_682e614d7eef44808486550b479ce5d2",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_475af4e8d7f34a5d9335bb972a36ec2b"
          }
        },
        "04058d4e69a5449e96762a5d6aa830d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0f71a5495854dcbb9f64b7fd3ad3d3c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 87.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7aa4a52ffb784034bd5bc083165bb45f"
          }
        },
        "682e614d7eef44808486550b479ce5d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "475af4e8d7f34a5d9335bb972a36ec2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0f71a5495854dcbb9f64b7fd3ad3d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7aa4a52ffb784034bd5bc083165bb45f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chirag-sharma-00/cs182-cv-project/blob/main/Adverserial_project_notebook_inception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VOqVl7R0Kuk"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import glob\n",
        "import pathlib\n",
        "import tqdm\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from google.colab import drive"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxdDCtGN0Xdd"
      },
      "source": [
        "!rm -rf sample_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ehVR2p-0Zrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab392ee-cd11-49b8-bc06-a5a494f77d41"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MSObgGtPqRZ"
      },
      "source": [
        "!unzip drive/MyDrive/182-cv-project/data/tiny-imagenet-200.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDowfYzc0mns"
      },
      "source": [
        "data_dir = pathlib.Path('tiny-imagenet-200/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSt6noAM0Kup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87220923-9956-4924-824e-b5ef5250a4c1"
      },
      "source": [
        "#image_count = len(list(data_dir.glob('**/*.JPEG')))\n",
        "!rm tiny-imagenet-200/.DS_Store\n",
        "!rm tiny-imagenet-200/train/.DS_Store\n",
        "!rm tiny-imagenet-200/val/.DS_Store\n",
        "!rm tiny-imagenet-200/test/.DS_Store"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'tiny-imagenet-200/.DS_Store': No such file or directory\n",
            "rm: cannot remove 'tiny-imagenet-200/train/.DS_Store': No such file or directory\n",
            "rm: cannot remove 'tiny-imagenet-200/val/.DS_Store': No such file or directory\n",
            "rm: cannot remove 'tiny-imagenet-200/test/.DS_Store': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17AQlMkS0Kum"
      },
      "source": [
        "## Preliminary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBLZ5xnC0Kun"
      },
      "source": [
        "images = pd.read_csv(\"tiny-imagenet-200/words.txt\", names = ['Id', 'labels'], sep = '\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLuV3ve10Kup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2325a5c0-cb14-4ef4-ce80-fcdb8787a5b8"
      },
      "source": [
        "CLASS_NAMES = np.array([item.name for item in (data_dir / 'train').glob('*')])\n",
        "CLASS_NAMES.sort()\n",
        "print(len(CLASS_NAMES)) #should be 200\n",
        "sum([cls in images['Id'].unique() for cls in CLASS_NAMES])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsZuTuOFRsJH"
      },
      "source": [
        "## Create augmented validation data folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y8VHeYBRvoV"
      },
      "source": [
        "import data_augmentation as aug\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOzCd2CkSmLH"
      },
      "source": [
        "#os.mkdir(\"tiny-imagenet-200/val/augmented_images\")\n",
        "\n",
        "prev_annotations = pd.read_csv(\"tiny-imagenet-200/val/val_annotations.txt\", \n",
        "                              sep='\\t', names=[\"Filename\", \"Class\", \"BB1\", \n",
        "                                               \"BB2\", \"BB3\", \"BB4\"])\n",
        "prev_annotations.drop([\"BB1\", \"BB2\", \"BB3\", \"BB4\"], axis=1, inplace=True)\n",
        "new_annotations = pd.DataFrame(columns=[\"Filename\", \"Class\"])\n",
        "\n",
        "for img_file in os.listdir(\"tiny-imagenet-200/val/images\"):\n",
        "  path = \"tiny-imagenet-200/val/images\" + \"/\" + img_file\n",
        "  augmented_imgs = aug.augmented_data_from_path(path)\n",
        "  for i, a in enumerate(augmented_imgs):\n",
        "    im = Image.fromarray(a)\n",
        "    prefix = img_file.split(\".\")[0]\n",
        "    im.save(\"tiny-imagenet-200/val/augmented_images/\" + prefix + \"_\" + str(i) + \n",
        "            \".JPEG\")\n",
        "    new_annotations = new_annotations.append({\"Filename\" : prefix + \"_\" + str(i) + \".JPEG\", \n",
        "                            \"Class\" : prev_annotations.loc[\n",
        "                              prev_annotations[\"Filename\"] == prefix + \".JPEG\",\n",
        "                              \"Class\"\n",
        "                            ].item()}, ignore_index=True)\n",
        "new_annotations.to_csv(\"tiny-imagenet-200/val/augmented_val_annotations.txt\", \n",
        "                       sep='\\t', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD2YikN7hNuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e8cab4-e9ee-4630-a99c-feef02d49599"
      },
      "source": [
        "!zip -r /content/augmented_images.zip /content/tiny-imagenet-200/val/augmented_images/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tzip warning: name not matched: /content/tiny-imagenet-200/val/augmented_images/\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r /content/augmented_images.zip . -i /content/tiny-imagenet-200/val/augmented_images/)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vISDVvN0Kuw"
      },
      "source": [
        "## Feature extraction/fine tuning model training code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKFgtKgi0Kuw"
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=1, is_inception=True):\n",
        "    if torch.cuda.is_available():\n",
        "      model = model.cuda()\n",
        "      \n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training (no val for now)\n",
        "        for phase in ['train','val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                # move to gpu\n",
        "                inputs = inputs.to(device)\n",
        "                labels.data = labels.data.to(device)\n",
        "                \n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUXDW_3u0Kux"
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting=True):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB1g-Zwt0Kuz"
      },
      "source": [
        "def predict(dataloaders, model): \n",
        "    \"\"\"\n",
        "    Run a forward pass (without caching data) for given model and return accuracy\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "      model = model.cuda()\n",
        "\n",
        "    accuracies = []\n",
        "    model.eval()\n",
        "    \n",
        "    for phase in tqdm.tqdm(['train', 'val']): \n",
        "        counter = 0\n",
        "        running_corrects = 0\n",
        "        running_total = 0\n",
        "\n",
        "        for inputs, labels in dataloaders[phase]: \n",
        "            inputs = inputs.to(device)\n",
        "            labels.data = labels.data.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            counter += 1\n",
        "            \n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            running_total += len(preds)\n",
        "            \n",
        "        phase_acc = running_corrects / running_total\n",
        "        print(phase_acc)\n",
        "        accuracies.append(phase_acc)\n",
        "\n",
        "    return accuracies\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QurZYF51A_lv"
      },
      "source": [
        "# Custom dataloader based on https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "class ValidationDataset(Dataset):\n",
        "  def __init__(self, annotations_file, img_dir, transform=None):\n",
        "    self.img_labels = pd.read_csv(annotations_file, sep='\\t', names=['image', 'label', 'x1', 'y1', 'x2', 'y2'])\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "    image = Image.open(img_path)\n",
        "    image = image.convert('RGB')\n",
        "    label = self.img_labels.iloc[idx, 1]\n",
        "    label = np.where(label==CLASS_NAMES)[0][0]\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dCrItwU--0L"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OXXBOpW0Kuq"
      },
      "source": [
        "## Testing Inception v3 out of the box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsVdHscI0Kur"
      },
      "source": [
        "data_dir = data_dir\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"inception_v3\"\n",
        "# Number of classes in the dataset\n",
        "num_classes = 200\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 8\n",
        "# Number of epochs to train for\n",
        "num_epochs = 3\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "# when True we only update the reshaped layer params\n",
        "feature_extract = True\n",
        "CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_kt9mOh0Kus",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "8db88b2d610f4bcca6687cd5f3b7b644",
            "6a63d66c1bbe43bbb7f5fd8fe14f10be",
            "67f0ab31af62465eb7a3267ba572f9f1",
            "88e01034a6864070901296630b4f2495",
            "e550025a59a347f184129038f56da8bd",
            "421c06cf93844b85b9648aafc24e8ae5",
            "cbbdcec1516f48d78bab6d2dadfdfe2e",
            "b8ecba4b76304b2d8aad2ea6fa054e5a"
          ]
        },
        "outputId": "8c38f997-376b-40ca-8dfc-ebe60e7f4f2a"
      },
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.9.0', model_name, pretrained=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/archive/v0.9.0.zip\" to /root/.cache/torch/hub/v0.9.0.zip\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8db88b2d610f4bcca6687cd5f3b7b644",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-7BA0Ej0Kut"
      },
      "source": [
        "#change last layers in model to match tiny imagenet\n",
        "model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
        "model.fc = nn.Linear(2048, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKmbeEje_mTr"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Y0EL_b0Kut"
      },
      "source": [
        "#inception expects input size 3*299*299\n",
        "input_size = 299"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqVB0yiOMQRj"
      },
      "source": [
        "### Feature extracting model performance on original validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7ZlghNbRPPf"
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), \n",
        "                                  data_transform),\n",
        "    'val': ValidationDataset(os.path.join(data_dir, 'val', 'val_annotations.txt'), \n",
        "                                      os.path.join(data_dir, 'val', 'images'), data_transform),\n",
        "}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                                   batch_size=batch_size, \n",
        "                                                   shuffle=True, num_workers=2) \n",
        "                                                   for x in image_datasets.keys()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTnk7HK6MPlM"
      },
      "source": [
        "num_classes = 200\n",
        "input_size = 299\n",
        "\n",
        "model_fe = models.inception_v3(pretrained=True)\n",
        "set_parameter_requires_grad(model_fe, feature_extract)\n",
        "# Handle the auxilary net\n",
        "num_ftrs = model_fe.AuxLogits.fc.in_features\n",
        "model_fe.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "# Handle the primary net\n",
        "num_ftrs = model_fe.fc.in_features\n",
        "model_fe.fc = nn.Linear(num_ftrs,num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6VTILInNsr_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26364892-4c4f-482a-87d3-87163d29d55f"
      },
      "source": [
        "# Send the model to GPU\n",
        "model_fe = model_fe.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. We will only update \n",
        "# the parameters that we have just initialized, i.e. the parameters with \n",
        "# requires_grad is True.\n",
        "print(\"Params to learn:\")\n",
        "params_to_update = []\n",
        "for name, param in model_fe.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t AuxLogits.fc.weight\n",
            "\t AuxLogits.fc.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIG2SNKyOVbx"
      },
      "source": [
        "optimizer_fe = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Train and evaluate\n",
        "model_fe, hist = train_model(model_fe, dataloaders_dict, criterion, \n",
        "                             optimizer_fe, num_epochs=num_epochs, \n",
        "                             is_inception=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdJ5g0FHpcBc"
      },
      "source": [
        "accuracies = predict(dataloaders_dict, model_fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuRjADiJZKE4"
      },
      "source": [
        "for i, key in enumerate(image_datasets.keys()):\n",
        "  print(key + \" accuracy = \", accuracies[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxSCiZ3S-VQL"
      },
      "source": [
        "torch.save(model_fe.state_dict(), 'cs182_project_models/oob-inception.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f07QM8lMQiaz"
      },
      "source": [
        "### Feature extracting model performance on augmented validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_oUefb7_AHx"
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), \n",
        "                                  data_transform),\n",
        "    'val': ValidationDataset(os.path.join(data_dir, 'val', 'augmented_val_annotations.txt'), \n",
        "                                      os.path.join(data_dir, 'val', 'augmented_val_images'), data_transform),\n",
        "}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                                   batch_size=batch_size, \n",
        "                                                   shuffle=True, num_workers=2) \n",
        "                                                   for x in image_datasets.keys()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "870PF2A-_Fic"
      },
      "source": [
        "accuracies = predict(dataloaders_dict, model_fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlaZBOE6_J22"
      },
      "source": [
        "for i, key in enumerate(image_datasets.keys()):\n",
        "  print(key + \" accuracy = \", accuracies[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQhmaFQp4AU9"
      },
      "source": [
        "## Fine-tuning the out of the box Inception v3 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rhpYo3J4WJh"
      },
      "source": [
        "data_dir = data_dir\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"inception_v3\"\n",
        "# Number of classes in the dataset\n",
        "num_classes = 200\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 32\n",
        "# Number of epochs to train for\n",
        "num_epochs = 3\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "# when True we only update the reshaped layer params\n",
        "feature_extract = True\n",
        "CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBvRasYj4WJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "a2c54a1960154758a06eeffe208f21b4",
            "bb7ad17fcbe54229a3fa5318ebf4cb0b",
            "365c6c1468cb4432af3de3aa0d0345a2",
            "ba6bdeba944e42198840898651c732c5",
            "f9dbb203bdb041e6bbd64ba6af899a33",
            "b3f1f8b6a2d94aac8716a074377fd11d",
            "793165e771684799be194b72048ac6b8",
            "a1cd452241d94440b7b8e54106790e07"
          ]
        },
        "outputId": "c86aa6cb-e4b2-472f-8541-674875ec759e"
      },
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.9.0', model_name, pretrained=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/archive/v0.9.0.zip\" to /root/.cache/torch/hub/v0.9.0.zip\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2c54a1960154758a06eeffe208f21b4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pz35OvA4WJj"
      },
      "source": [
        "#change last layers in model to match tiny imagenet\n",
        "model.AuxLogits.fc = nn.Linear(768, num_classes)\n",
        "model.fc = nn.Linear(2048, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGJ80f8L4WJk"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_qYr7MS4WJk"
      },
      "source": [
        "#inception expects input size 3*299*299\n",
        "input_size = 299"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2j0RmF2QcNk"
      },
      "source": [
        "### Fine-tuned model performance on original validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLM9tQPLQlsf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "76733ea8-0a78-4902-f902-5ce77313e528"
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), \n",
        "                                  data_transform),\n",
        "    'val': ValidationDataset(os.path.join(data_dir, 'val', 'val_annotations.txt'), \n",
        "                                      os.path.join(data_dir, 'val', 'images'), data_transform),\n",
        "}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                                   batch_size=batch_size, \n",
        "                                                   shuffle=True, num_workers=2) \n",
        "                                                   for x in image_datasets.keys()}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-31e2d6153c05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), \n\u001b[1;32m      9\u001b[0m                                   data_transform),\n\u001b[0;32m---> 10\u001b[0;31m     'val': ValidationDataset(os.path.join(data_dir, 'val', 'val_annotations.txt'), \n\u001b[0m\u001b[1;32m     11\u001b[0m                                       os.path.join(data_dir, 'val', 'images'), data_transform),\n\u001b[1;32m     12\u001b[0m }\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ValidationDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT6V3tnAQlsi"
      },
      "source": [
        "num_classes = 200\n",
        "input_size = 299\n",
        "\n",
        "model_fe = models.inception_v3(pretrained=True)\n",
        "set_parameter_requires_grad(model_fe, not feature_extract)\n",
        "# Handle the auxilary net\n",
        "num_ftrs = model_fe.AuxLogits.fc.in_features\n",
        "model_fe.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "# Handle the primary net\n",
        "num_ftrs = model_fe.fc.in_features\n",
        "model_fe.fc = nn.Linear(num_ftrs,num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx2ISMw6Qlsi"
      },
      "source": [
        "# Send the model to GPU\n",
        "model_fe = model_fe.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. We will only update \n",
        "# the parameters that we have just initialized, i.e. the parameters with \n",
        "# requires_grad is True.\n",
        "print(\"Params to learn:\")\n",
        "params_to_update = []\n",
        "for name, param in model_fe.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJYDWjHQQlsj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b345afc-5967-4333-bfc5-a7cd85e818ed"
      },
      "source": [
        "optimizer_fe = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Train and evaluate\n",
        "model_fe, hist = train_model(model_fe, dataloaders_dict, criterion, \n",
        "                             optimizer_fe, num_epochs=num_epochs, \n",
        "                             is_inception=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/2\n",
            "----------\n",
            "train Loss: 3.3031 Acc: 0.5644\n",
            "val Loss: 0.9213 Acc: 0.7682\n",
            "\n",
            "Epoch 1/2\n",
            "----------\n",
            "train Loss: 1.3897 Acc: 0.7857\n",
            "val Loss: 0.8355 Acc: 0.7884\n",
            "\n",
            "Epoch 2/2\n",
            "----------\n",
            "train Loss: 0.9278 Acc: 0.8555\n",
            "val Loss: 0.8725 Acc: 0.7807\n",
            "\n",
            "Training complete in 45m 4s\n",
            "Best val Acc: 0.788400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2hGMZrCQlsj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c8218c-1360-408d-efbf-1b0787efc144"
      },
      "source": [
        "accuracies = predict(dataloaders_dict, model_fe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [05:11<05:11, 311.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.8961, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [05:42<00:00, 171.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7884, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLivZ_jaQlsj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ff3c50-1592-467f-e98e-3480c28c8029"
      },
      "source": [
        "for i, key in enumerate(image_datasets.keys()):\n",
        "  print(key + \" accuracy = \", accuracies[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train accuracy =  tensor(0.8961, device='cuda:0')\n",
            "val accuracy =  tensor(0.7884, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gErPBNOOQlsj"
      },
      "source": [
        "torch.save(model_fe.state_dict(), '/content/drive/MyDrive/cs182_project_models/fine-tuned-inception.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av9DAMpYRGyY"
      },
      "source": [
        "### Fine-tuned model performance on augmented validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWGQzOeMRGyZ"
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), \n",
        "                                  data_transform),\n",
        "    'val': ValidationDataset(os.path.join(data_dir, 'val', 'augmented_val_annotations.txt'), \n",
        "                                      os.path.join(data_dir, 'val', 'augmented_val_images'), data_transform),\n",
        "}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                                   batch_size=batch_size, \n",
        "                                                   shuffle=True, num_workers=2) \n",
        "                                                   for x in image_datasets.keys()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqAKEHIxRGyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b0b8fb8-3654-457c-de9b-50c438645594"
      },
      "source": [
        "accuracies = predict(dataloaders_dict, model_fe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 1/2 [05:08<05:08, 308.68s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.8961, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 2/2 [05:40<00:00, 170.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.6098, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noUhZq7sRGya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66deaad2-2990-4927-f6d3-df6efb2312f7"
      },
      "source": [
        "for i, key in enumerate(image_datasets.keys()):\n",
        "  print(key + \" accuracy = \", accuracies[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train accuracy =  tensor(0.8961, device='cuda:0')\n",
            "val accuracy =  tensor(0.6098, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K42ZC7WOHcLW"
      },
      "source": [
        "###Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKHYUszLHbIx"
      },
      "source": [
        "#load finetuned model\n",
        "#model.load_state_dict(torch.load('drive/MyDrive/cs182_project_models/fine-tuned-augmented-train-inception.pt'))\n",
        "model.load_state_dict(torch.load('drive/MyDrive/chet_models/fine-tuned-inception.pt'))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYNSoBfFHl0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500c08fb-61a5-405f-bd2a-ab2f386d6265"
      },
      "source": [
        "!pip install torchattacks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchattacks\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/55/91c60b07daa4538090db811f75a1ab99b6d3db8342965027d76fab361dc7/torchattacks-2.14.2-py3-none-any.whl (92kB)\n",
            "\r\u001b[K     |███▌                            | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 20kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 30kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 40kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 51kB 3.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 61kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 71kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 81kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 92kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 4.0MB/s \n",
            "\u001b[?25hInstalling collected packages: torchattacks\n",
            "Successfully installed torchattacks-2.14.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUDYS23MHudi"
      },
      "source": [
        "import torchattacks\n",
        "atks = [torchattacks.FGSM(model, eps=8/255),\n",
        "        torchattacks.PGD(model, eps=8/255, alpha=2/255, steps=7),\n",
        "        #torchattacks.Square(model, eps=8/255),\n",
        "       ]\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHiaj9zhIFgJ"
      },
      "source": [
        "def train_model_adv(model, dataloaders, criterion, optimizer, num_epochs=3, is_inception=True):\n",
        "    \"\"\"\n",
        "    Takes a pretrained model and trains it using a unique adversarial attack for each epoch\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "      model = model.cuda()\n",
        "      \n",
        "    since = time.time()\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train','val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                # move to gpu\n",
        "                inputs = inputs.to(device)\n",
        "                #adv attack\n",
        "                atk = random.choice(atks)\n",
        "                inputs = atk(inputs, labels)\n",
        "\n",
        "                labels.data = labels.data.to(device)\n",
        "                \n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "            torch.save(model_fe.state_dict(), 'drive/MyDrive/chet_models/adverserial-trained-inception-randomized.pt')\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8hQ47V7LBst"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_fe = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_YuQwC0K3tN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88cb801b-056a-4823-ec20-ef5dc7bdc590"
      },
      "source": [
        "model_adv, hist = train_model_adv(model_fe, dataloaders_dict, criterion, \n",
        "                             optimizer_fe, num_epochs=3, \n",
        "                             is_inception=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/2\n",
            "----------\n",
            "train Loss: 1.0585 Acc: 0.8281\n",
            "val Loss: 1.4337 Acc: 0.6702\n",
            "Epoch 1/2\n",
            "----------\n",
            "train Loss: 0.8172 Acc: 0.8700\n",
            "val Loss: 1.4781 Acc: 0.6699\n",
            "Epoch 2/2\n",
            "----------\n",
            "train Loss: 0.6425 Acc: 0.9000\n",
            "val Loss: 1.5659 Acc: 0.6604\n",
            "Training complete in 218m 16s\n",
            "Best val Acc: 0.670200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DglCR4ROL3F"
      },
      "source": [
        "###Test on Imagenet-a, Imagenet-o data and original Val data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8j2gBldZKi_"
      },
      "source": [
        "#merge imagenet-a folder with a copy of tiny-imagenet-200\n",
        "!mkdir tiny-imageneta/\n",
        "!cp -R tiny-imagenet-200/ tiny-imageneta/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMxomcnVZlyy"
      },
      "source": [
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiM_o07iZ6wR"
      },
      "source": [
        "!unzip drive/MyDrive/182-cv-project/data/imagenet-a.zip\n",
        "imagenet_a_path = pathlib.Path('imagenet-a/')\n",
        "!unzip drive/MyDrive/182-cv-project/data/imagenet-o.zip\n",
        "imagenet_o_path = pathlib.Path('imagenet-o/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp4LGIMRLO6t"
      },
      "source": [
        "imagenet_a_classes = np.array([item.name for item in (imagenet_a_path).glob('*')])\n",
        "imagenet_o_classes = np.array([item.name for item in (imagenet_o_path).glob('*')])\n",
        "\n",
        "take_imga = [clas for clas in imagenet_a_classes if clas in CLASS_NAMES]\n",
        "take_imgo = [clas for clas in imagenet_o_classes if clas in CLASS_NAMES]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiluqQ4eaX-6"
      },
      "source": [
        "#copy files from imagenet-a into tiny-imageneta train folder\n",
        "for dir in take_imga: \n",
        "  source_path = os.path.join(imagenet_a_path, dir)\n",
        "  dest_path = os.path.join(pathlib.Path('tiny-imageneta/tiny-imagenet-200/train/'), dir)\n",
        "\n",
        "  for file in os.listdir(source_path):\n",
        "    try: \n",
        "      shutil.copy(os.path.join(source_path, file), dest_path)\n",
        "    except: \n",
        "      continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkRpd_EIfFQ9"
      },
      "source": [
        "#merge imagenet-o folder with a copy of tiny-imagenet-200\n",
        "!mkdir tiny-imageneto/\n",
        "!cp -R tiny-imagenet-200/ tiny-imageneto/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g8v5Ab1fFRD"
      },
      "source": [
        "#copy files from imagenet-a into tiny-imageneta train folder\n",
        "for dir in take_imgo: \n",
        "  source_path = os.path.join(imagenet_o_path, dir)\n",
        "  dest_path = os.path.join(pathlib.Path('tiny-imageneto/tiny-imagenet-200/train/'), dir)\n",
        "\n",
        "  for file in os.listdir(source_path):\n",
        "    try: \n",
        "      shutil.copy(os.path.join(source_path, file), dest_path)\n",
        "    except: \n",
        "      continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfjgVrus_3a-"
      },
      "source": [
        "#load adversarial model, non randomized\n",
        "model.load_state_dict(torch.load('drive/MyDrive/cs182_project_models/cs182_project_models/adverserial-trained-inception-no-square.pt'))\n",
        "model.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPer7F57BKT3"
      },
      "source": [
        "#Test adv trained model on original validation data\n",
        "data_transform = transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), \n",
        "                                  data_transform),\n",
        "    'val': ValidationDataset(os.path.join(data_dir, 'val', 'val_annotations.txt'), \n",
        "                                      os.path.join(data_dir, 'val', 'images'), data_transform),\n",
        "}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                                   batch_size=batch_size, \n",
        "                                                   shuffle=True, num_workers=2) \n",
        "                                                   for x in image_datasets.keys()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uJ_MjDoBZ2k",
        "outputId": "35be58f8-fbd8-4e8a-b462-fdcec8f483d5"
      },
      "source": [
        "orig_val_accuracies = predict(dataloaders_dict, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [17:12<17:12, 1032.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.3627, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [18:56<00:00, 568.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.3233, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXhZINLBBo_y"
      },
      "source": [
        "#Test adv model on augmented validation data\n",
        "data_transform = transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), \n",
        "                                  data_transform),\n",
        "    'val': ValidationDataset(os.path.join(data_dir, 'val', 'augmented_val_annotations.txt'), \n",
        "                                      os.path.join(data_dir, 'val', 'augmented_val_images'), data_transform),\n",
        "}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                                   batch_size=batch_size, \n",
        "                                                   shuffle=True, num_workers=2) \n",
        "                                                   for x in image_datasets.keys()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuIEy0iMDC06"
      },
      "source": [
        "aug_val_accuracies = predict(dataloaders_dict, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOrzLDmUB2g2"
      },
      "source": [
        "#Test model on imagenet-a, unseen test dataset\n",
        "data_transform = transforms.Compose([\n",
        "        #https://discuss.pytorch.org/t/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-3-224-224-at-entry-0-and-3-224-336-at-entry-3/87211/9\n",
        "        transforms.Resize((input_size, input_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), \n",
        "    ])\n",
        "\n",
        "#train and val are the same, just that the predict function expection train and val keys \n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(imagenet_a_path, \n",
        "                                  data_transform),\n",
        "    'val': datasets.ImageFolder(imagenet_a_path, \n",
        "                                  data_transform)\n",
        "}\n",
        "\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                                   batch_size=batch_size, \n",
        "                                                   shuffle=True, num_workers=2) \n",
        "                                                   for x in image_datasets.keys()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqiX9vISC7Ju",
        "outputId": "4f7afcdd-f077-4ffc-eaae-887556fc845c"
      },
      "source": [
        "imagenet_a_accuracies = predict(dataloaders_dict, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1/2 [00:31<00:31, 31.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0010, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [01:03<00:00, 31.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0010, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7O_i47yJ7ON"
      },
      "source": [
        "#Test model on imagenet-o\n",
        "data_transform = transforms.Compose([\n",
        "        #https://discuss.pytorch.org/t/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-3-224-224-at-entry-0-and-3-224-336-at-entry-3/87211/9\n",
        "        transforms.Resize((input_size, input_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), \n",
        "    ])\n",
        "\n",
        "#train and val are the same, just that the predict function expection train and val keys \n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(imagenet_o_path, \n",
        "                                  data_transform),\n",
        "    'val': datasets.ImageFolder(imagenet_o_path, \n",
        "                                  data_transform)\n",
        "}\n",
        "\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                                   batch_size=batch_size, \n",
        "                                                   shuffle=True, num_workers=2) \n",
        "                                                   for x in image_datasets.keys()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mrNFmTBVzIs"
      },
      "source": [
        "imagenet_o_accuracies = predict(dataloaders_dict, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tFtyIEMi5DE"
      },
      "source": [
        "###Test ResNet50 OOB on imagenet-a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfFkah3AjQVs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "929804dc09e746f08672c4c6c9b9de17",
            "39ebbdf8ad2942adb49b27d1a14d8b7e",
            "6c345bbcd2aa44609a2fc1184810432e",
            "04058d4e69a5449e96762a5d6aa830d3",
            "682e614d7eef44808486550b479ce5d2",
            "475af4e8d7f34a5d9335bb972a36ec2b",
            "d0f71a5495854dcbb9f64b7fd3ad3d3c",
            "7aa4a52ffb784034bd5bc083165bb45f"
          ]
        },
        "outputId": "ab0d6d52-1269-43cd-a781-eeb3de27e6ad"
      },
      "source": [
        "model_resnet = torch.hub.load('pytorch/vision:v0.9.0', 'resnet50', pretrained=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.9.0\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "929804dc09e746f08672c4c6c9b9de17",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZevLtc4Vi_gl"
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "        #https://discuss.pytorch.org/t/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-3-224-224-at-entry-0-and-3-224-336-at-entry-3/87211/9\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), \n",
        "    ])\n",
        "\n",
        "#train and val are the same, just that the predict function expection train and val keys \n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(imagenet_a_path, \n",
        "                                  data_transform),\n",
        "    'val': datasets.ImageFolder(imagenet_a_path, \n",
        "                                  data_transform)\n",
        "}\n",
        "\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                                   batch_size=2, \n",
        "                                                   shuffle=True, num_workers=2) \n",
        "                                                   for x in image_datasets.keys()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVuJmnXpjpMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc905229-7af7-4358-f20b-2b687e82c239"
      },
      "source": [
        "predict(dataloaders_dict, model_resnet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|█████     | 1/2 [00:40<00:40, 40.70s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0010, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "100%|██████████| 2/2 [01:21<00:00, 40.60s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0010, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(0.0010, device='cuda:0'), tensor(0.0010, device='cuda:0')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOpamVONjtPI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a409864-00d3-437e-9ee9-ea32f6087c64"
      },
      "source": [
        "sum([1 for dir in os.listdir(imagenet_o_path)])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilcxoukmlkL8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}